{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfdcf103",
   "metadata": {},
   "source": [
    "## Import modules and create Spark session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdfcafd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import module\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import SparkSession\n",
    "#create session\n",
    "appName = \"Recommender system in Spark\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(appName) \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99860eee",
   "metadata": {},
   "source": [
    "## Read file into dataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd7b551",
   "metadata": {},
   "source": [
    "5.\tLoad the u.data, and u.user files into Apache Spark as DataFrames named df_udata and df_uuser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d5ed5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read file into dataFrame using automatically inferred schema\n",
    "df_udata = spark.read.csv('D:/New folder (3)/movielens/u_data.csv', inferSchema=True, header=True)\n",
    "df_uuser = spark.read.csv('D:/New folder (3)/movielens/u_user.csv', inferSchema=True, header=True)\n",
    "df_uuser = df_uuser.selectExpr('user_id as userid','age','gender','occupation','zip_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "173b7539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------+---------+\n",
      "|user_id|item_id|rating|timestamp|\n",
      "+-------+-------+------+---------+\n",
      "|    196|    242|     3|881250949|\n",
      "|    186|    302|     3|891717742|\n",
      "+-------+-------+------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_udata.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eba0909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, user_id: string, item_id: string, rating: string, timestamp: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_udata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "370b9e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+----------+--------+\n",
      "|userid|age|gender|occupation|zip_code|\n",
      "+------+---+------+----------+--------+\n",
      "|     1| 24|     M|technician|   85711|\n",
      "|     2| 53|     F|     other|   94043|\n",
      "+------+---+------+----------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_uuser.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb72d07",
   "metadata": {},
   "source": [
    "a.\tHow many unique occupations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eec04fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|   occupation|\n",
      "+-------------+\n",
      "|administrator|\n",
      "|       artist|\n",
      "|       doctor|\n",
      "|     educator|\n",
      "|     engineer|\n",
      "|entertainment|\n",
      "|    executive|\n",
      "|   healthcare|\n",
      "|    homemaker|\n",
      "|       lawyer|\n",
      "|    librarian|\n",
      "|    marketing|\n",
      "|         none|\n",
      "|        other|\n",
      "|   programmer|\n",
      "|      retired|\n",
      "|     salesman|\n",
      "|    scientist|\n",
      "|      student|\n",
      "|   technician|\n",
      "|       writer|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_uuser.select(\"occupation\");\n",
    "\n",
    "unique= df_uuser.select('occupation').distinct()\n",
    "\n",
    "unique.orderBy('occupation').show(100)\n",
    "unique.orderBy('occupation').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5511a01a",
   "metadata": {},
   "source": [
    "what is the frequency of each occupation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5c9418c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+\n",
      "|   Occupation|Frequency|\n",
      "+-------------+---------+\n",
      "|    homemaker|        7|\n",
      "|       doctor|        7|\n",
      "|         none|        9|\n",
      "|       lawyer|       12|\n",
      "|     salesman|       12|\n",
      "|      retired|       14|\n",
      "|   healthcare|       16|\n",
      "|entertainment|       18|\n",
      "|    marketing|       26|\n",
      "|   technician|       27|\n",
      "|       artist|       28|\n",
      "|    scientist|       31|\n",
      "|    executive|       32|\n",
      "|       writer|       45|\n",
      "|    librarian|       51|\n",
      "|   programmer|       66|\n",
      "|     engineer|       67|\n",
      "|administrator|       79|\n",
      "|     educator|       95|\n",
      "|        other|      105|\n",
      "|      student|      196|\n",
      "+-------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "freq = df_uuser.groupBy('occupation').count()\n",
    "freq = freq.selectExpr('occupation as Occupation', 'count as Frequency')\n",
    "freq = freq.orderBy('Frequency').show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf47c03",
   "metadata": {},
   "source": [
    "# Find the number of recommendations corresponding to each occupation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb963f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, user_id: string, item_id: string, gender: string, age: string, rating: string, occupation: string, zip_code: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Recommendation = df_udata.join(df_uuser,df_udata.user_id == df_uuser.userid)\n",
    "Recommendation = Recommendation.select('user_id','item_id','gender','age','rating','occupation','zip_code')\n",
    "Recommendation.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3730ef4",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a01dfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training data rows: 69818 , number of testing data rows: 30182\n"
     ]
    }
   ],
   "source": [
    "#use only column data of \"userId\", \"movieId\", dan \"rating\"\n",
    "data = Recommendation.select(\"user_id\", \"item_id\", \"rating\")\n",
    "\n",
    "#divide data, 70% for training and 30% for testing\n",
    "splits = data.randomSplit([0.7, 0.3])\n",
    "train = splits[0].withColumnRenamed(\"rating\", \"label\")\n",
    "test = splits[1].withColumnRenamed(\"rating\", \"trueLabel\")\n",
    "\n",
    "#calculate number of rows\n",
    "train_rows = train.count()\n",
    "test_rows = test.count()\n",
    "print (\"number of training data rows:\", train_rows, \", number of testing data rows:\", test_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c60f6c3",
   "metadata": {},
   "source": [
    "## Define model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d07d6ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n"
     ]
    }
   ],
   "source": [
    "#define ALS (Alternating Least Square) as our recommender system\n",
    "als = ALS(maxIter=19, regParam=0.01, userCol=\"user_id\",itemCol=\"item_id\", ratingCol=\"label\")\n",
    "#train our ALS model\n",
    "model = als.fit(train)\n",
    "model =model.setColdStartStrategy(\"drop\");\n",
    "print(\"Training is done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff87c90",
   "metadata": {},
   "source": [
    "## Predict testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8c68161",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(test)\n",
    "# prediction.show()\n",
    "# prediction = predictions.selectExpr(\"user_id as userid\",\"item_id\",\"trueLable\",\"predictions\")\n",
    "# print(\"testing is done!\")\n",
    "# prediction = prediction.selectExpr('user_id as id','item_id','trueLabel','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49011077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[item_id: int, user_id: int, trueLabel: int, prediction: float, user_id: int, rating: int, timestamp: int]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.join(df_udata, \"item_id\")#.show(n=5, truncate=False)\n",
    "\n",
    "# .select(\"user_id\", \"occupation\", \"gender\").show(n=10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c29efea",
   "metadata": {},
   "source": [
    "## Evaluate the accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c105dadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error (RMSE): 1.1414492988449596\n"
     ]
    }
   ],
   "source": [
    "#import RegressionEvaluator since we also want to calculate RMSE (Root Mean Square Error)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "model =model.setColdStartStrategy(\"drop\")\n",
    "rmse = evaluator.evaluate(prediction)\n",
    "print (\"Root Mean Square Error (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27ac4912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of original data rows:  30123\n",
      "number of rows after dropping data with missing value:  30123\n",
      "number of missing data:  0\n"
     ]
    }
   ],
   "source": [
    "prediction.count()\n",
    "a = prediction.count()\n",
    "print(\"number of original data rows: \", a)\n",
    "#drop rows with any missing data\n",
    "cleanPred = prediction.dropna(how=\"any\", subset=[\"prediction\"])\n",
    "b = cleanPred.count()\n",
    "print(\"number of rows after dropping data with missing value: \", b)\n",
    "print(\"number of missing data: \", a-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80203d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Square Error (RMSE): 1.1414492988449607\n"
     ]
    }
   ],
   "source": [
    "rmse = evaluator.evaluate(cleanPred)\n",
    "print (\"Root Mean Square Error (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a4988eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "user1 = test.filter(test['user_id'] == 1).select(['user_id','item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fc14abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+\n",
      "|user_id|item_id|prediction|\n",
      "+-------+-------+----------+\n",
      "|      1|     53| 5.7811384|\n",
      "|      1|    152| 5.2109337|\n",
      "|      1|    250| 5.0954776|\n",
      "|      1|    168|   5.08208|\n",
      "|      1|     92| 5.0252438|\n",
      "|      1|    224|  4.956689|\n",
      "|      1|    115| 4.7802987|\n",
      "|      1|    156|  4.759315|\n",
      "|      1|     59|  4.702451|\n",
      "|      1|    268|  4.625274|\n",
      "+-------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rec = model.transform(user1)\n",
    "rec.orderBy('prediction',ascending = False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86d59554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zubiar shams\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "UserRecommendation = model.recommendForAllUsers(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a803516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UserRecommendation.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6aa65088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = false)\n",
      " |-- recommendations: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- item_id: integer (nullable = true)\n",
      " |    |    |-- rating: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "UserRecommendation.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b67d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "UserRecommendationfinal=UserRecommendation.selectExpr('user_id as user_id','recommendations.item_id as Recommended_Movies')\n",
    "# .show(30,False)\n",
    "UserRecommendationfinal.describe();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9241a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uuser= df_uuser.selectExpr('userid AS user_id','occupation','age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "285752d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = (df_uuser.join(UserRecommendationfinal,['user_id'],'inner'))\n",
    "\n",
    "# .selectExpr('user_id','recommendations.item_id as Recommended_Movies').show(30,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92ad9b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------------------------------+\n",
      "|user_id|occupation   |Recommended_Movies_IDs        |\n",
      "+-------+-------------+------------------------------+\n",
      "|1      |technician   |[880, 1093, 1184, 1069, 1311] |\n",
      "|3      |writer       |[6, 953, 1311, 458, 695]      |\n",
      "|5      |other        |[253, 1094, 899, 1589, 834]   |\n",
      "|6      |executive    |[1368, 1204, 1643, 320, 865]  |\n",
      "|12     |other        |[1159, 960, 1456, 915, 502]   |\n",
      "|13     |educator     |[574, 974, 557, 837, 955]     |\n",
      "|16     |entertainment|[865, 630, 1107, 530, 1066]   |\n",
      "|19     |librarian    |[601, 1204, 497, 493, 644]    |\n",
      "|20     |homemaker    |[989, 1297, 580, 1213, 1389]  |\n",
      "|22     |writer       |[989, 422, 130, 1085, 1120]   |\n",
      "|26     |engineer     |[1643, 1598, 64, 1169, 605]   |\n",
      "|27     |librarian    |[962, 634, 74, 276, 6]        |\n",
      "|28     |writer       |[1589, 1169, 1269, 1483, 1218]|\n",
      "|31     |artist       |[1205, 1005, 916, 1065, 962]  |\n",
      "|34     |administrator|[1643, 1176, 865, 1166, 1134] |\n",
      "|40     |scientist    |[1159, 915, 836, 1024, 959]   |\n",
      "|44     |technician   |[1643, 426, 865, 960, 1279]   |\n",
      "|47     |marketing    |[1426, 1120, 1069, 745, 352]  |\n",
      "|48     |administrator|[630, 1219, 1159, 915, 1107]  |\n",
      "|52     |student      |[6, 251, 1266, 464, 224]      |\n",
      "|53     |programmer   |[502, 1404, 1195, 295, 700]   |\n",
      "|54     |executive    |[865, 667, 796, 1483, 1159]   |\n",
      "|57     |none         |[1483, 1160, 353, 1159, 1218] |\n",
      "|64     |educator     |[1218, 922, 745, 1483, 630]   |\n",
      "|65     |educator     |[1279, 502, 960, 1160, 1159]  |\n",
      "|76     |student      |[320, 57, 865, 1266, 1449]    |\n",
      "|78     |administrator|[916, 108, 1204, 805, 1473]   |\n",
      "|81     |student      |[753, 915, 532, 922, 865]     |\n",
      "|85     |educator     |[865, 320, 464, 968, 1069]    |\n",
      "|86     |administrator|[1245, 1120, 1313, 1266, 130] |\n",
      "+-------+-------------+------------------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.selectExpr('user_id','occupation','Recommended_Movies as Recommended_Movies_IDs').show(30,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca7e50ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Recommended_Movies: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- colcounter: integer (nullable = false)\n",
      "\n",
      "+-------+-------------+---+--------------------+----------+\n",
      "|user_id|   occupation|age|  Recommended_Movies|colcounter|\n",
      "+-------+-------------+---+--------------------+----------+\n",
      "|      1|   technician| 24|[880, 1093, 1184,...|         5|\n",
      "|      3|       writer| 23|[6, 953, 1311, 45...|         5|\n",
      "|      5|        other| 33|[253, 1094, 899, ...|         5|\n",
      "|      6|    executive| 42|[1368, 1204, 1643...|         5|\n",
      "|     12|        other| 28|[1159, 960, 1456,...|         5|\n",
      "|     13|     educator| 47|[574, 974, 557, 8...|         5|\n",
      "|     16|entertainment| 21|[865, 630, 1107, ...|         5|\n",
      "|     19|    librarian| 40|[601, 1204, 497, ...|         5|\n",
      "|     20|    homemaker| 42|[989, 1297, 580, ...|         5|\n",
      "|     22|       writer| 25|[989, 422, 130, 1...|         5|\n",
      "|     26|     engineer| 49|[1643, 1598, 64, ...|         5|\n",
      "|     27|    librarian| 40|[962, 634, 74, 27...|         5|\n",
      "|     28|       writer| 32|[1589, 1169, 1269...|         5|\n",
      "|     31|       artist| 24|[1205, 1005, 916,...|         5|\n",
      "|     34|administrator| 38|[1643, 1176, 865,...|         5|\n",
      "|     40|    scientist| 38|[1159, 915, 836, ...|         5|\n",
      "|     44|   technician| 26|[1643, 426, 865, ...|         5|\n",
      "|     47|    marketing| 53|[1426, 1120, 1069...|         5|\n",
      "|     48|administrator| 45|[630, 1219, 1159,...|         5|\n",
      "|     52|      student| 18|[6, 251, 1266, 46...|         5|\n",
      "+-------+-------------+---+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " from pyspark.sql.functions import *\n",
    "# final2 = final.withColumn(\"col5\", final[\"Recommended_Movies\"].getItem(1)).withColumn(\"Recommended_Movies\", final[\"Recommended_Movies\"].getItem(0))\n",
    "final3 = final.withColumn(\"colcounter\", size(final[\"Recommended_Movies\"]))\n",
    "final3.printSchema()\n",
    "final3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a05e7ccc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'colcounter' given input columns: [occupation, totalNumberOfMovies];\n'Aggregate [occupation#43], [occupation#43, count('colcounter) AS totalNumberOfMovies#4155]\n+- Aggregate [occupation#43], [occupation#43, count(colcounter#3692) AS totalNumberOfMovies#4136L]\n   +- Project [user_id#1888, occupation#43, age#41, Recommended_Movies#1763, size(Recommended_Movies#1763, true) AS colcounter#3692]\n      +- Project [user_id#1888, occupation#43, age#41, Recommended_Movies#1763]\n         +- Join Inner, (user_id#1888 = user_id#1762)\n            :- Project [userid#50 AS user_id#1888, occupation#43, age#41]\n            :  +- Project [user_id#40 AS userid#50, age#41, gender#42, occupation#43, zip_code#44]\n            :     +- Relation [user_id#40,age#41,gender#42,occupation#43,zip_code#44] csv\n            +- Project [user_id#1758 AS user_id#1762, recommendations#1759.item_id AS Recommended_Movies#1763]\n               +- Project [id#1754 AS user_id#1758, cast(recommendations#1755 as array<struct<item_id:int,rating:float>>) AS recommendations#1759]\n                  +- Project [key#1748 AS id#1754, TopByKeyAggregator(scala.Tuple3)#1753 AS recommendations#1755]\n                     +- Aggregate [value#1740], [value#1740 AS key#1748, topbykeyaggregator(org.apache.spark.ml.recommendation.TopByKeyAggregator@358934b9, Some(newInstance(class scala.Tuple3)), Some(class scala.Tuple3), Some(StructType(StructField(_1,IntegerType,false), StructField(_2,IntegerType,false), StructField(_3,FloatType,false))), encodeusingserializer(input[0, java.lang.Object, true], true), decodeusingserializer(input[0, binary, true], org.apache.spark.util.BoundedPriorityQueue, true), mapobjects(lambdavariable(MapObject, ObjectType(class scala.Tuple2), true, 37), if (isnull(lambdavariable(MapObject, ObjectType(class scala.Tuple2), true, 37))) null else named_struct(_1, knownnotnull(lambdavariable(MapObject, ObjectType(class scala.Tuple2), true, 37))._1, _2, knownnotnull(lambdavariable(MapObject, ObjectType(class scala.Tuple2), true, 37))._2), input[0, [Lscala.Tuple2;, true], None), ArrayType(StructType(StructField(_1,IntegerType,false), StructField(_2,FloatType,false)),true), true, 0, 0) AS TopByKeyAggregator(scala.Tuple3)#1753]\n                        +- AppendColumns org.apache.spark.ml.recommendation.ALSModel$$Lambda$4180/1069984848@33e6f3fa, class scala.Tuple3, [StructField(_1,IntegerType,false), StructField(_2,IntegerType,false), StructField(_3,FloatType,false)], newInstance(class scala.Tuple3), [input[0, int, false] AS value#1740]\n                           +- SerializeFromObject [knownnotnull(assertnotnull(input[0, scala.Tuple3, true]))._1 AS _1#1729, knownnotnull(assertnotnull(input[0, scala.Tuple3, true]))._2 AS _2#1730, knownnotnull(assertnotnull(input[0, scala.Tuple3, true]))._3 AS _3#1731]\n                              +- MapPartitions org.apache.spark.ml.recommendation.ALSModel$$Lambda$4178/290297471@16c2b579, obj#1728: scala.Tuple3\n                                 +- DeserializeToObject newInstance(class scala.Tuple4), obj#1727: scala.Tuple4\n                                    +- Join Cross\n                                       :- SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(IntegerType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1, true, false) AS _1#1698, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, true, false) AS _2#1699]\n                                       :  +- MapPartitions org.apache.spark.ml.recommendation.ALSModel$$Lambda$4175/1183495715@bee075c, obj#1697: scala.Tuple2\n                                       :     +- DeserializeToObject newInstance(class scala.Tuple2), obj#1696: scala.Tuple2\n                                       :        +- Project [_1#1497 AS id#1502, _2#1498 AS features#1503]\n                                       :           +- SerializeFromObject [knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1 AS _1#1497, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, true, false) AS _2#1498]\n                                       :              +- ExternalRDD [obj#1496]\n                                       +- SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(IntegerType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1, true, false) AS _1#1709, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, true, false) AS _2#1710]\n                                          +- MapPartitions org.apache.spark.ml.recommendation.ALSModel$$Lambda$4175/1183495715@7ac1b8f2, obj#1708: scala.Tuple2\n                                             +- DeserializeToObject newInstance(class scala.Tuple2), obj#1707: scala.Tuple2\n                                                +- Project [_1#1509 AS id#1514, _2#1510 AS features#1515]\n                                                   +- SerializeFromObject [knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1 AS _1#1509, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, true, false) AS _2#1510]\n                                                      +- ExternalRDD [obj#1508]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m final3 \u001b[38;5;241m=\u001b[39m \u001b[43mfinal3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moccupation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolcounter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotalNumberOfMovies\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# .alias(\"totalNumberOfRecommendadMovies\")\u001b[39;00m\n\u001b[0;32m      3\u001b[0m final3\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moccupation\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m30\u001b[39m,\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\users\\zubiar shams\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pyspark\\sql\\group.py:118\u001b[0m, in \u001b[0;36mGroupedData.agg\u001b[1;34m(self, *exprs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# Columns\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall exprs should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 118\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[1;32mc:\\users\\zubiar shams\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\users\\zubiar shams\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pyspark\\sql\\utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: cannot resolve 'colcounter' given input columns: [occupation, totalNumberOfMovies];\n'Aggregate [occupation#43], [occupation#43, count('colcounter) AS totalNumberOfMovies#4155]\n+- Aggregate [occupation#43], [occupation#43, count(colcounter#3692) AS totalNumberOfMovies#4136L]\n   +- Project [user_id#1888, occupation#43, age#41, Recommended_Movies#1763, size(Recommended_Movies#1763, true) AS colcounter#3692]\n      +- Project [user_id#1888, occupation#43, age#41, Recommended_Movies#1763]\n         +- Join Inner, (user_id#1888 = user_id#1762)\n            :- Project [userid#50 AS user_id#1888, occupation#43, age#41]\n            :  +- Project [user_id#40 AS userid#50, age#41, gender#42, occupation#43, zip_code#44]\n            :     +- Relation [user_id#40,age#41,gender#42,occupation#43,zip_code#44] csv\n            +- Project [user_id#1758 AS user_id#1762, recommendations#1759.item_id AS Recommended_Movies#1763]\n               +- Project [id#1754 AS user_id#1758, cast(recommendations#1755 as array<struct<item_id:int,rating:float>>) AS recommendations#1759]\n                  +- Project [key#1748 AS id#1754, TopByKeyAggregator(scala.Tuple3)#1753 AS recommendations#1755]\n                     +- Aggregate [value#1740], [value#1740 AS key#1748, topbykeyaggregator(org.apache.spark.ml.recommendation.TopByKeyAggregator@358934b9, Some(newInstance(class scala.Tuple3)), Some(class scala.Tuple3), Some(StructType(StructField(_1,IntegerType,false), StructField(_2,IntegerType,false), StructField(_3,FloatType,false))), encodeusingserializer(input[0, java.lang.Object, true], true), decodeusingserializer(input[0, binary, true], org.apache.spark.util.BoundedPriorityQueue, true), mapobjects(lambdavariable(MapObject, ObjectType(class scala.Tuple2), true, 37), if (isnull(lambdavariable(MapObject, ObjectType(class scala.Tuple2), true, 37))) null else named_struct(_1, knownnotnull(lambdavariable(MapObject, ObjectType(class scala.Tuple2), true, 37))._1, _2, knownnotnull(lambdavariable(MapObject, ObjectType(class scala.Tuple2), true, 37))._2), input[0, [Lscala.Tuple2;, true], None), ArrayType(StructType(StructField(_1,IntegerType,false), StructField(_2,FloatType,false)),true), true, 0, 0) AS TopByKeyAggregator(scala.Tuple3)#1753]\n                        +- AppendColumns org.apache.spark.ml.recommendation.ALSModel$$Lambda$4180/1069984848@33e6f3fa, class scala.Tuple3, [StructField(_1,IntegerType,false), StructField(_2,IntegerType,false), StructField(_3,FloatType,false)], newInstance(class scala.Tuple3), [input[0, int, false] AS value#1740]\n                           +- SerializeFromObject [knownnotnull(assertnotnull(input[0, scala.Tuple3, true]))._1 AS _1#1729, knownnotnull(assertnotnull(input[0, scala.Tuple3, true]))._2 AS _2#1730, knownnotnull(assertnotnull(input[0, scala.Tuple3, true]))._3 AS _3#1731]\n                              +- MapPartitions org.apache.spark.ml.recommendation.ALSModel$$Lambda$4178/290297471@16c2b579, obj#1728: scala.Tuple3\n                                 +- DeserializeToObject newInstance(class scala.Tuple4), obj#1727: scala.Tuple4\n                                    +- Join Cross\n                                       :- SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(IntegerType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1, true, false) AS _1#1698, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, true, false) AS _2#1699]\n                                       :  +- MapPartitions org.apache.spark.ml.recommendation.ALSModel$$Lambda$4175/1183495715@bee075c, obj#1697: scala.Tuple2\n                                       :     +- DeserializeToObject newInstance(class scala.Tuple2), obj#1696: scala.Tuple2\n                                       :        +- Project [_1#1497 AS id#1502, _2#1498 AS features#1503]\n                                       :           +- SerializeFromObject [knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1 AS _1#1497, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, true, false) AS _2#1498]\n                                       :              +- ExternalRDD [obj#1496]\n                                       +- SerializeFromObject [staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(IntegerType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1, true, false) AS _1#1709, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, true, false) AS _2#1710]\n                                          +- MapPartitions org.apache.spark.ml.recommendation.ALSModel$$Lambda$4175/1183495715@7ac1b8f2, obj#1708: scala.Tuple2\n                                             +- DeserializeToObject newInstance(class scala.Tuple2), obj#1707: scala.Tuple2\n                                                +- Project [_1#1509 AS id#1514, _2#1510 AS features#1515]\n                                                   +- SerializeFromObject [knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._1 AS _1#1509, staticinvoke(class org.apache.spark.sql.catalyst.expressions.UnsafeArrayData, ArrayType(FloatType,false), fromPrimitiveArray, knownnotnull(assertnotnull(input[0, scala.Tuple2, true]))._2, true, false) AS _2#1510]\n                                                      +- ExternalRDD [obj#1508]\n"
     ]
    }
   ],
   "source": [
    "final3 = final3.groupBy(\"occupation\").agg(count(\"colcounter\").alias('totalNumberOfMovies'))\n",
    "# .alias(\"totalNumberOfRecommendadMovies\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d813d796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+\n",
      "|occupation   |totalNumberOfMovies|\n",
      "+-------------+-------------------+\n",
      "|administrator|79                 |\n",
      "|artist       |28                 |\n",
      "|doctor       |7                  |\n",
      "|educator     |95                 |\n",
      "|engineer     |67                 |\n",
      "|entertainment|18                 |\n",
      "|executive    |32                 |\n",
      "|healthcare   |16                 |\n",
      "|homemaker    |7                  |\n",
      "|lawyer       |12                 |\n",
      "|librarian    |51                 |\n",
      "|marketing    |26                 |\n",
      "|none         |9                  |\n",
      "|other        |105                |\n",
      "|programmer   |66                 |\n",
      "|retired      |14                 |\n",
      "|salesman     |12                 |\n",
      "|scientist    |31                 |\n",
      "|student      |196                |\n",
      "|technician   |27                 |\n",
      "|writer       |45                 |\n",
      "+-------------+-------------------+\n",
      "\n",
      "Total Unique Number of occupation  ---> 21\n"
     ]
    }
   ],
   "source": [
    "final3.orderBy('occupation').show(30,False)\n",
    "unique= final3.select('occupation').distinct()\n",
    "print('Total Unique Number of occupation  --->', unique.orderBy('occupation').count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
